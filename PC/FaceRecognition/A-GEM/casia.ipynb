{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cal-05/anaconda3/envs/hspark/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.util import get_image_list, dataset_list, PathDataset, PathDataset_v2, make_task\n",
    "from avalanche.benchmarks.datasets import CelebA\n",
    "\n",
    "from avalanche.benchmarks.utils import AvalancheTensorDataset\n",
    "from avalanche.training.supervised import AGEM\n",
    "from avalanche.logging import InteractiveLogger, WandBLogger\n",
    "from avalanche.benchmarks.generators import nc_benchmark, paths_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset, AvalancheDatasetType\n",
    "from avalanche.benchmarks.utils.dataset_definitions import IDatasetWithTargets\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "from avalanche.evaluation.metrics import ExperienceAccuracy, ExperienceLoss, ExperienceForgetting, ExperienceCPUUsage, ExperienceMaxGPU, ExperienceMaxRAM, ExperienceTime, EpochAccuracy\n",
    "from avalanche.benchmarks.generators import (\n",
    "    filelist_scenario, dataset_scenario, tensor_scenario, paths_scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "cudnn.deterministic = True  # 연산 처리 속도 감소 -> 모델과 코드를 배포해야 하는 연구 후반 단계에 사용\n",
    "cudnn.benchmark = False\n",
    "# torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  66487\n",
      "test :  16622\n"
     ]
    }
   ],
   "source": [
    "incremental = 500\n",
    "data_path = \"../Data/CASIA-WebFace/\" \n",
    "\n",
    "dataset = get_image_list(data_path)\n",
    "\n",
    "image_dataset = []\n",
    "label_dataset = []\n",
    "incremental_list = list(range(incremental))\n",
    "\n",
    "for idx, label in enumerate(dataset[1]):\n",
    "    if label in incremental_list:\n",
    "        image_dataset.append(dataset[0][idx])\n",
    "        label_dataset.append(dataset[1][idx])\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size=0.2, shuffle=True, random_state=seed, stratify=dataset[1])\n",
    "x_train, x_test, y_train, y_test = train_test_split(image_dataset, label_dataset, test_size=0.2, shuffle=True, random_state=seed, stratify=label_dataset)\n",
    "\n",
    "train_set = dataset_list(x_train, y_train)\n",
    "test_set = dataset_list(x_test, y_test)\n",
    "\n",
    "print(\"train : \", len(y_train))\n",
    "print(\"test : \", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_group = dict(\n",
    "       train=(\n",
    "       transforms.Compose(\n",
    "              [\n",
    "              transforms.ToTensor(),\n",
    "              ]\n",
    "       ),\n",
    "       None,\n",
    "       ),\n",
    "       eval=(\n",
    "       transforms.Compose(\n",
    "              [\n",
    "              transforms.ToTensor(),\n",
    "              ]\n",
    "       ),\n",
    "       None,\n",
    "       )\n",
    ")\n",
    "\n",
    "# train_set = AvalancheDataset(trian_loader)\n",
    "# test_set = AvalancheDataset(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# num_class = 10575\n",
    "# train_task = make_task(incremental, y_train)\n",
    "# test_task = make_task(incremental, y_test)\n",
    "\n",
    "# train_set = PathDataset(train_set, transforms_group, \"train\", train_task)\n",
    "# test_set = PathDataset(test_set, transforms_group, \"eval\", test_task)\n",
    "train_set = PathDataset_v2(train_set, transforms_group, \"train\")\n",
    "test_set = PathDataset_v2(test_set, transforms_group, \"eval\")\n",
    "\n",
    "scenario = nc_benchmark(train_dataset=train_set,\n",
    "                        test_dataset=test_set,\n",
    "                        n_experiences=incremental,\n",
    "                        task_labels=False,\n",
    "                        seed=seed,\n",
    "                        shuffle=True\n",
    "                        )\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False, num_classes=incremental)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhopo55\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "interactive_logger = InteractiveLogger()\n",
    "wandb_logger = WandBLogger(project_name=\"Face Recognition Test\", run_name=\"AGEM-CASIA-M10-E10-C500\")\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    EpochAccuracy(),\n",
    "    ExperienceAccuracy(),\n",
    "    ExperienceLoss(),\n",
    "    ExperienceForgetting(),\n",
    "    ExperienceTime(),\n",
    "    loggers=[interactive_logger, wandb_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 10\n",
    "eval_batch = 1\n",
    "epoch = 10\n",
    "memory_size = 10\n",
    "\n",
    "strategies = AGEM(model, optimizer, criterion, patterns_per_exp=memory_size, sample_size=memory_size, train_epochs=epoch, device=device, train_mb_size=train_batch, eval_mb_size=eval_batch, evaluator=eval_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "for experience in scenario.train_stream:\n",
    "    print(\"Start training on experience \", experience.current_experience)\n",
    "    strategies.train(experience)\n",
    "    print(\"End training on experience \", experience.current_experience)\n",
    "    print(\"Computing accuracy on the test set\")\n",
    "    results.append(strategies.eval(scenario.test_stream[:]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7200c97f9580f269913cae0521a31c8c9bc2a022b66e7eee6f7caa3cb908faad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('hspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a4110d7404e15bb29e9af898bb3709c8ad5c91368ac9d283907772f906a4946"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
