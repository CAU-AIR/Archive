{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB\\Anaconda3\\envs\\lab\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\LAB\\Anaconda3\\envs\\lab\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\LAB\\Anaconda3\\envs\\lab\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\LAB\\Anaconda3\\envs\\lab\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Users\\LAB\\Anaconda3\\envs\\lab\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.util import get_image_list, ImageDataset, dataset_list, PathDataset, PathDataset_v2, make_task\n",
    "from avalanche.benchmarks.datasets import CelebA\n",
    "\n",
    "from avalanche.benchmarks.utils import AvalancheTensorDataset\n",
    "from avalanche.training.supervised import GEM\n",
    "from avalanche.logging import InteractiveLogger, WandBLogger\n",
    "from avalanche.benchmarks.generators import nc_benchmark, paths_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset, AvalancheDatasetType\n",
    "from avalanche.benchmarks.utils.dataset_definitions import IDatasetWithTargets\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "from avalanche.evaluation.metrics import ExperienceAccuracy, ExperienceLoss, ExperienceForgetting, ExperienceCPUUsage, ExperienceMaxGPU, ExperienceMaxRAM, ExperienceTime, EpochAccuracy\n",
    "from avalanche.benchmarks.generators import (\n",
    "    filelist_scenario, dataset_scenario, tensor_scenario, paths_scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "cudnn.deterministic = True  # 연산 처리 속도 감소 -> 모델과 코드를 배포해야 하는 연구 후반 단계에 사용\n",
    "cudnn.benchmark = False\n",
    "# torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  395531\n",
      "test :  98883\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../Data/CASIA-WebFace/\"\n",
    "\n",
    "dataset = get_image_list(data_path)\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size=0.2, shuffle=True, random_state=seed, stratify=dataset[1])\n",
    "\n",
    "train_set = dataset_list(x_train, y_train)\n",
    "test_set = dataset_list(x_test, y_test)\n",
    "\n",
    "# train_set = ImageDataset(x_train, y_train)\n",
    "# test_set = ImageDataset(x_test, y_test)\n",
    "# train_dataset = TensorDataset(x_train, y_train)\n",
    "# test_dataset = TensorDataset(x_test, y_test)\n",
    "# train_dataset = AvalancheTensorDataset(x_train, y_train)\n",
    "# test_dataset = AvalancheTensorDataset(x_test, y_test)\n",
    "# train_set = tensor_scenario(x_train, y_train, task_labels=0)\n",
    "# test_set = tensor_scenario(x_test, y_test)\n",
    "\n",
    "print(\"train : \", len(y_train))\n",
    "print(\"test : \", len(y_test))\n",
    "\n",
    "# print(\"target : \", train_set.traget)\n",
    "\n",
    "# # check avalanche dataset\n",
    "# x,y = train_set[0]\n",
    "# print(\"avalanche dataset x : \", type(x))\n",
    "# print(\"avalanche dataset y : \", type(y))\n",
    "\n",
    "## Data imbalance\n",
    "# from collections import Counter\n",
    "# cnt = Counter(dataset[1])\n",
    "# print(\"Target : \", cnt.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_group = dict(\n",
    "       train=(\n",
    "       transforms.Compose(\n",
    "              [\n",
    "              transforms.ToTensor(),\n",
    "              ]\n",
    "       ),\n",
    "       None,\n",
    "       ),\n",
    "       eval=(\n",
    "       transforms.Compose(\n",
    "              [\n",
    "              transforms.ToTensor(),\n",
    "              ]\n",
    "       ),\n",
    "       None,\n",
    "       )\n",
    ")\n",
    "\n",
    "# train_set = AvalancheDataset(trian_loader)\n",
    "# test_set = AvalancheDataset(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_class = 10575\n",
    "incremental = 5\n",
    "# task_lable = [0,1,2,3,4]\n",
    "# task_lable = list(range(10575))\n",
    "# train_task = make_task(incremental, y_train)\n",
    "# test_task = make_task(incremental, y_test)\n",
    "\n",
    "# train_set = PathDataset(train_set, transforms_group, \"train\", train_task)\n",
    "# test_set = PathDataset(test_set, transforms_group, \"eval\", test_task)\n",
    "train_set = PathDataset_v2(train_set, transforms_group, \"train\")\n",
    "test_set = PathDataset_v2(test_set, transforms_group, \"eval\")\n",
    "\n",
    "# scenario = paths_benchmark(train_lists_of_files=train_set,\n",
    "#                            test_lists_of_files=test_set,\n",
    "#                            task_labels=task_lable,\n",
    "#                            complete_test_set_only=True,\n",
    "#                            train_transform=transform,\n",
    "#                            eval_transform=transform,\n",
    "#                            dataset_type=AvalancheDatasetType.CLASSIFICATION\n",
    "# )\n",
    "\n",
    "scenario = nc_benchmark(train_dataset=train_set,\n",
    "                        test_dataset=test_set,\n",
    "                        n_experiences=incremental,\n",
    "                        task_labels=True,\n",
    "                        seed=seed,\n",
    "                        shuffle=False\n",
    "                        )\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False, num_classes=num_class)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhopo55\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\LAB\\Desktop\\Archive\\PC\\FaceRecognition\\GEM\\wandb\\run-20220727_011805-2t4blpag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hopo55/Face%20Recognition/runs/2t4blpag\" target=\"_blank\">GEM-CASIA-M64</a></strong> to <a href=\"https://wandb.ai/hopo55/Face%20Recognition\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB\\Anaconda3\\envs\\lab\\lib\\site-packages\\avalanche\\training\\plugins\\evaluation.py:81: UserWarning: No benchmark provided to the evaluation plugin. Metrics may be computed on inconsistent portion of streams, use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "interactive_logger = InteractiveLogger()\n",
    "wandb_logger = WandBLogger(project_name=\"Face Recognition\", run_name=\"GEM-CASIA-M64\")\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    EpochAccuracy(),\n",
    "    ExperienceAccuracy(),\n",
    "    ExperienceLoss(),\n",
    "    ExperienceForgetting(),\n",
    "    ExperienceCPUUsage(),\n",
    "    ExperienceMaxGPU(gpu_id=0),\n",
    "    ExperienceMaxRAM(),\n",
    "    ExperienceTime(),\n",
    "    loggers=[interactive_logger, wandb_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 10\n",
    "eval_batch = 1\n",
    "epoch = 1\n",
    "\n",
    "strategies = GEM(model, optimizer, criterion, patterns_per_exp=64, memory_strength=0.5, train_epochs=epoch, device=device, train_mb_size=train_batch, eval_mb_size=eval_batch, evaluator=eval_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start training on experience  0\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 11693/11693 [07:23<00:00, 26.35it/s]\n",
      "Epoch 0 ended.\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0289\n",
      "-- >> End of training phase << --\n",
      "End training on experience  0\n",
      "Computing accuracy on the test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 29221/29221 [03:30<00:00, 139.14it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 109.5960\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.4131\n",
      "\tMaxGPU0Usage_Experience/eval_phase/test_stream/Task000/Exp000 = 54.0000\n",
      "\tMaxRAMUsage_Experience/eval_phase/test_stream/Task000/Exp000 = 4410.2148\n",
      "\tTime_Exp/eval_phase/test_stream/Task000/Exp000 = 210.0168\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0375\n",
      "-- Starting eval on experience 1 (Task 1) from test stream --\n",
      "100%|██████████| 20168/20168 [02:25<00:00, 138.37it/s]\n",
      "> Eval on experience 1 (Task 1) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task001/Exp001 = 109.4366\n",
      "\tLoss_Exp/eval_phase/test_stream/Task001/Exp001 = 12.7308\n",
      "\tMaxGPU0Usage_Experience/eval_phase/test_stream/Task001/Exp001 = 44.0000\n",
      "\tMaxRAMUsage_Experience/eval_phase/test_stream/Task001/Exp001 = 4410.2070\n",
      "\tTime_Exp/eval_phase/test_stream/Task001/Exp001 = 145.7572\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task001/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 2) from test stream --\n",
      "100%|██████████| 19523/19523 [02:21<00:00, 137.71it/s]\n",
      "> Eval on experience 2 (Task 2) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task002/Exp002 = 110.5505\n",
      "\tLoss_Exp/eval_phase/test_stream/Task002/Exp002 = 12.7489\n",
      "\tMaxGPU0Usage_Experience/eval_phase/test_stream/Task002/Exp002 = 47.0000\n",
      "\tMaxRAMUsage_Experience/eval_phase/test_stream/Task002/Exp002 = 4410.2188\n",
      "\tTime_Exp/eval_phase/test_stream/Task002/Exp002 = 141.7687\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task002/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 3) from test stream --\n",
      "100%|██████████| 16313/16313 [01:57<00:00, 139.08it/s]\n",
      "> Eval on experience 3 (Task 3) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task003/Exp003 = 108.8685\n",
      "\tLoss_Exp/eval_phase/test_stream/Task003/Exp003 = 12.6973\n",
      "\tMaxGPU0Usage_Experience/eval_phase/test_stream/Task003/Exp003 = 43.0000\n",
      "\tMaxRAMUsage_Experience/eval_phase/test_stream/Task003/Exp003 = 4410.2227\n",
      "\tTime_Exp/eval_phase/test_stream/Task003/Exp003 = 117.2905\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task003/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 4) from test stream --\n",
      "100%|██████████| 13658/13658 [01:40<00:00, 135.76it/s]\n",
      "> Eval on experience 4 (Task 4) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task004/Exp004 = 109.3555\n",
      "\tLoss_Exp/eval_phase/test_stream/Task004/Exp004 = 12.5819\n",
      "\tMaxGPU0Usage_Experience/eval_phase/test_stream/Task004/Exp004 = 50.0000\n",
      "\tMaxRAMUsage_Experience/eval_phase/test_stream/Task004/Exp004 = 4410.2266\n",
      "\tTime_Exp/eval_phase/test_stream/Task004/Exp004 = 100.6044\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task004/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "Start training on experience  1\n",
      "-- >> Start of training phase << --\n",
      " 42%|████▏     | 3424/8065 [12:58<17:17,  4.47it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "for experience in scenario.train_stream:\n",
    "    print(\"Start training on experience \", experience.current_experience)\n",
    "    strategies.train(experience)\n",
    "    print(\"End training on experience \", experience.current_experience)\n",
    "    print(\"Computing accuracy on the test set\")\n",
    "    results.append(strategies.eval(scenario.test_stream[:]))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, exp in enumerate(scenario.train_stream):\n",
    "#     eval_exps = [e for e in scenario.test_stream][: i + 1]\n",
    "#     strategies.train(exp)\n",
    "#     strategies.eval(eval_exps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a4110d7404e15bb29e9af898bb3709c8ad5c91368ac9d283907772f906a4946"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
