{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CNN and NCM for MNIST feature extraction and classification\n",
    "### Summary\n",
    "MNIST 데이터셋을 분류하기 위해 먼저 CNN을 Pretraining 시킨 후, FC layer만 제거하여 Feature Extractor로 활용.\n",
    "\n",
    "Feature Extractor로써 CNN만 추가한 것과 PCA or LDA를 추가하여 NCM을 실험한 결과들."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Load MNIST Dataset](#load-mnist-dataset)\n",
    "* [Pretraining CNN](#pretraining-cnn)\n",
    "* [64 Feature Extractor (CNN)](#64-feature-extrator-cnn)\n",
    "* [CNN feature + NCM](#cnn-feature--ncm)\n",
    "* [CNN feature + PCA + NCM](#cnn-feature--pca--ncm)\n",
    "  * [PCA run function](#pca-run-function)\n",
    "  * [CNN features - 2 PCA - NCM](#cnn-features---2-pca---ncm)\n",
    "  * [CNN features - 3 PCA - NCM](#cnn-features---3-pca---ncm)\n",
    "  * [CNN features - 5 PCA - NCM](#cnn-features---5-pca---ncm)\n",
    "  * [CNN features - 26 PCA - NCM](#cnn-features---26-pca---ncm)\n",
    "  * [CNN features - 43 PCA - NCM](#cnn-features---43-pca---ncm)\n",
    "* [CNN feature + LDA + NCM](#cnn-feature--lda--ncm)\n",
    "  * [LDA run function](#lda-run-function)\n",
    "  * [CNN features - 2 LDA - NCM](#cnn-features---2-lda---ncm)\n",
    "  * [CNN features - 3 LDA - NCM](#cnn-features---3-lda---ncm)\n",
    "  * [CNN features - 5 LDA - NCM](#cnn-features---5-lda---ncm)\n",
    "  * [CNN features - 9 LDA - NCM](#cnn-features---9-lda---ncm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from utils.utils import Info, accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from utils.NCM_Classifier import train, validate\n",
    "# from models.resnet_feature import resnet18_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(Info):\n",
    "    def __init__(self):\n",
    "        super(Info, self).__init__()\n",
    "        self.device = 'PC'\n",
    "        self.dataset = 'MNIST'\n",
    "        self.test_size = 0.2\n",
    "        self.feature_size = 784\n",
    "        self.method = 'NCM'\n",
    "        self.distance = 'Euclidean'\n",
    "        self.reduction_method = [None, None] # method, n_components\n",
    "        self.iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: None\n",
      "│    └────Component size: None\n",
      "│    └────Feature Reduction Ratio: None%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n",
      "PC - MNIST(80%) - NCM - 10 iteration\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.info()\n",
    "config.print_rutin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "cudnn.deterministic = True  # 연산 처리 속도 감소 -> 모델과 코드를 배포해야 하는 연구 후반 단계에 사용\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cau_13\\Anaconda3\\envs\\tf-venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ])\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../../datasets', train=True, download=False, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "validationset = torchvision.datasets.MNIST(root='../../datasets', train=False, download=False, transform=transform)\n",
    "val_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(trainset.data.shape)\n",
    "print(validationset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x): # 28 x 28\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x) # 14 x 14\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x) # 7 x 7\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x) # 4 x 4\n",
    "\n",
    "        x = self.avgpool(x) # 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : Train-118, Test-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = CNN().to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "total_train_batch = len(train_loader)\n",
    "total_test_batch = len(val_loader)\n",
    "print('총 배치의 수 : Train-{}, Test-{}'.format(total_train_batch, total_test_batch))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] Acc : 76.8609848022461 cost : 0.881799221\n",
      "Test Acc: 94.0647964477539\n",
      "[Epoch:    2] Acc : 94.56600189208984 cost : 0.183034062\n",
      "Test Acc: 96.19657897949219\n",
      "[Epoch:    3] Acc : 96.1384506225586 cost : 0.128635019\n",
      "Test Acc: 96.94622802734375\n",
      "[Epoch:    4] Acc : 96.85572814941406 cost : 0.105369374\n",
      "Test Acc: 96.46771240234375\n",
      "[Epoch:    5] Acc : 97.2474365234375 cost : 0.0903387964\n",
      "Test Acc: 98.08306884765625\n",
      "[Epoch:    6] Acc : 97.62645721435547 cost : 0.0771759674\n",
      "Test Acc: 97.93658447265625\n",
      "[Epoch:    7] Acc : 97.84662628173828 cost : 0.0704659894\n",
      "Test Acc: 98.34558868408203\n",
      "[Epoch:    8] Acc : 98.05178833007812 cost : 0.0641076788\n",
      "Test Acc: 98.35650634765625\n",
      "[Epoch:    9] Acc : 98.21627044677734 cost : 0.0577964783\n",
      "Test Acc: 98.41165161132812\n",
      "[Epoch:   10] Acc : 98.2978515625 cost : 0.0547632203\n",
      "Test Acc: 98.32605743408203\n",
      "[Epoch:   11] Acc : 98.52849578857422 cost : 0.0497457087\n",
      "Test Acc: 98.66670227050781\n",
      "[Epoch:   12] Acc : 98.55992889404297 cost : 0.0472592115\n",
      "Test Acc: 98.54951477050781\n",
      "[Epoch:   13] Acc : 98.70005798339844 cost : 0.0429967344\n",
      "Test Acc: 98.60064697265625\n",
      "[Epoch:   14] Acc : 98.68682861328125 cost : 0.0421352088\n",
      "Test Acc: 98.20771789550781\n",
      "[Epoch:   15] Acc : 98.74144744873047 cost : 0.0408906266\n",
      "Test Acc: 98.65693664550781\n",
      "[Epoch:   16] Acc : 98.78223419189453 cost : 0.0379887\n",
      "Test Acc: 98.57996368408203\n",
      "[Epoch:   17] Acc : 98.94501495361328 cost : 0.0359597094\n",
      "Test Acc: 98.81318664550781\n",
      "[Epoch:   18] Acc : 98.98310852050781 cost : 0.0327225849\n",
      "Test Acc: 98.72644805908203\n",
      "[Epoch:   19] Acc : 99.00186920166016 cost : 0.0319777168\n",
      "Test Acc: 98.39441680908203\n",
      "[Epoch:   20] Acc : 99.00955963134766 cost : 0.0311180539\n",
      "Test Acc: 98.77412414550781\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    avg_train_acc = 0\n",
    "    avg_test_acc = 0\n",
    "\n",
    "    net.train()\n",
    "    for X, Y in train_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y느 ㄴ레이블.\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = net(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        acc = accuracy(hypothesis, Y)\n",
    "\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_train_batch\n",
    "        avg_train_acc += acc / total_train_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] Acc : {:>4} cost : {:>.9}'.format(epoch + 1, avg_train_acc.item(), avg_cost.item()))\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_test, Y_test in val_loader:\n",
    "            X_test = X_test.to(device)\n",
    "            Y_test = Y_test.to(device)\n",
    "\n",
    "            prediction = net(X_test)\n",
    "\n",
    "            acc = accuracy(prediction, Y_test)\n",
    "            avg_test_acc += acc / total_test_batch\n",
    "\n",
    "    print('Test Acc: {:>4}'.format(avg_test_acc.item()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64 Feature Extrator (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "net.fc = Identity()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN feature + NCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN feature extract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cnn_feature(net, \n",
    "                       train_loader, \n",
    "                       val_loader):\n",
    "\n",
    "    def extract_feature(loader):\n",
    "        features = []\n",
    "        targets = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i, (images, target) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            feature = net(images)\n",
    "            features.extend(feature.cpu().numpy())\n",
    "            targets.extend(target.cpu().numpy())\n",
    "        feature_extract_time = time.time() - start_time\n",
    "        features = np.array(features)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        return features, targets, feature_extract_time\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Train dataset\n",
    "        train_features, train_targets, feature_extract_time = extract_feature(train_loader)\n",
    "        # Test dataset\n",
    "        test_features, test_targets, test_feature_extract_time = extract_feature(val_loader)\n",
    "        \n",
    "    return train_features, train_targets, test_features, test_targets, feature_extract_time, test_feature_extract_time\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets, test_features, test_targets, \\\n",
    "feature_extract_time, test_feature_extract_time = extract_cnn_feature(net, \n",
    "                                                                      train_loader=train_loader, \n",
    "                                                                      val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + NCM run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ncm_run(train_features,\n",
    "            train_targets,\n",
    "            test_features,\n",
    "            test_targets,\n",
    "            config = None,\n",
    "            metric = 'euclidean'):\n",
    "\n",
    "    classifier = NearestCentroid(metric=metric)\n",
    "\n",
    "    # Train dataset\n",
    "    c_start_time = time.time()\n",
    "    classifier.fit(train_features, train_targets)\n",
    "    fit_time = time.time() - c_start_time\n",
    "    output = classifier.predict(train_features)\n",
    "\n",
    "    # measure accuracy\n",
    "    acc = accuracy_score(output, train_targets)\n",
    "\n",
    "    # Test dataset\n",
    "    c_start_time = time.time()\n",
    "    output = classifier.predict(test_features)\n",
    "    predict_time = time.time() - c_start_time\n",
    "\n",
    "    # measure accuracy\n",
    "    test_acc = accuracy_score(output, test_targets)\n",
    "\n",
    "    print('\\nFinished Fit\\n')\n",
    "    print(\"Train Data Feature Extract Time : %.4f\" % feature_extract_time, \"sec\")\n",
    "    print(\"Train Data Fitting NCM Time : %.4f\" % fit_time, \"sec\")\n",
    "    print(\"Train Accuracy : %.2f\" % (acc*100), \"%\")\n",
    "    print('\\nFinished Predicting\\n')\n",
    "    print(\"Test Data Feature Extract Time : %.4f\" % test_feature_extract_time, \"sec\")\n",
    "    print(\"Test Data Prediction Time : %.4f\" % predict_time, \"sec\")\n",
    "    print(\"Test Accuracy : %.2f\" % (test_acc*100), \"%\")\n",
    "\n",
    "    return acc, test_acc, feature_extract_time, test_feature_extract_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: None\n",
      "│    └────Component size: None\n",
      "│    └────Feature Reduction Ratio: None%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0140 sec\n",
      "Train Accuracy : 94.62 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0040 sec\n",
      "Test Accuracy : 95.39 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(train_features, \n",
    "                train_targets, \n",
    "                test_features, \n",
    "                test_targets,\n",
    "                config = config,\n",
    "                metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN feature + PCA + NCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_run(train_data_X,\n",
    "            test_data_X,\n",
    "            config=None):\n",
    "\n",
    "    assert config is not None\n",
    "#     config.info()\n",
    "\n",
    "    n_components = config.reduction_method[1]\n",
    "\n",
    "    pca_dims = PCA(n_components)\n",
    "    print(f\"The number of components : {n_components}\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    pca_dims.fit(train_data_X)\n",
    "    pca_fit_time = time.perf_counter () - start_time\n",
    "    print()\n",
    "    print(f\"Calculating SVD Matrix Time on Train Data-{train_data_X.shape} : {pca_fit_time:4f} sec\")\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    train_features = pca_dims.transform(train_data_X)\n",
    "    train_features_extract_time = time.perf_counter () - start_time\n",
    "    print(f\"Transform train X-{train_data_X.shape} to {n_components}-PCA Time: {train_features_extract_time:4f} sec\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_features = pca_dims.transform(test_data_X)\n",
    "    test_features_extract_time = time.perf_counter () - start_time\n",
    "    print(f\"Transform test X-{train_data_X.shape} to {n_components}-PCA Time: {test_features_extract_time:4f} sec\")\n",
    "\n",
    "    return pca_dims, train_features, test_features, pca_fit_time, train_features_extract_time, test_features_extract_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 2 PCA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: PCA\n",
      "│    └────Component size: 2\n",
      "│    └────Feature Reduction Ratio: 0.3%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['PCA', 2]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 2\n",
      "\n",
      "Calculating SVD Matrix Time on Train Data-(60000, 64) : 0.057550 sec\n",
      "Transform train X-(60000, 64) to 2-PCA Time: 0.007760 sec\n",
      "Transform test X-(60000, 64) to 2-PCA Time: 0.001408 sec\n"
     ]
    }
   ],
   "source": [
    "pca_dims, pca_train_features, pca_test_features, pca_fit_time, pca_train_features_extract_time, pca_test_features_extract_time = \\\n",
    "    pca_run(train_data_X = train_features,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0060 sec\n",
      "Train Accuracy : 55.73 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 56.21 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(pca_train_features, \n",
    "                train_targets, \n",
    "                pca_test_features, \n",
    "                test_targets,\n",
    "                config = config,\n",
    "                metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 3 PCA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: PCA\n",
      "│    └────Component size: 3\n",
      "│    └────Feature Reduction Ratio: 0.4%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['PCA', 3]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 3\n",
      "\n",
      "Calculating SVD Matrix Time on Train Data-(60000, 64) : 0.067082 sec\n",
      "Transform train X-(60000, 64) to 3-PCA Time: 0.011019 sec\n",
      "Transform test X-(60000, 64) to 3-PCA Time: 0.002021 sec\n"
     ]
    }
   ],
   "source": [
    "pca_dims, pca_train_features, pca_test_features, pca_fit_time, pca_train_features_extract_time, pca_test_features_extract_time = \\\n",
    "    pca_run(train_data_X = train_features,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0060 sec\n",
      "Train Accuracy : 71.13 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0020 sec\n",
      "Test Accuracy : 72.16 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(pca_train_features, \n",
    "                train_targets, \n",
    "                pca_test_features, \n",
    "                test_targets,\n",
    "                config = config,\n",
    "                metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 5 PCA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: PCA\n",
      "│    └────Component size: 5\n",
      "│    └────Feature Reduction Ratio: 0.6%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['PCA', 5]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 5\n",
      "\n",
      "Calculating SVD Matrix Time on Train Data-(60000, 64) : 0.076544 sec\n",
      "Transform train X-(60000, 64) to 5-PCA Time: 0.007974 sec\n",
      "Transform test X-(60000, 64) to 5-PCA Time: 0.002129 sec\n"
     ]
    }
   ],
   "source": [
    "pca_dims, pca_train_features, pca_test_features, pca_fit_time, pca_train_features_extract_time, pca_test_features_extract_time = \\\n",
    "    pca_run(train_data_X = train_features,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0050 sec\n",
      "Train Accuracy : 85.73 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 87.23 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(pca_train_features, \n",
    "                train_targets, \n",
    "                pca_test_features, \n",
    "                test_targets,\n",
    "                config = config,\n",
    "                metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 26 PCA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: PCA\n",
      "│    └────Component size: 26\n",
      "│    └────Feature Reduction Ratio: 3.3000000000000003%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['PCA', 26]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 26\n",
      "\n",
      "Calculating SVD Matrix Time on Train Data-(60000, 64) : 0.103076 sec\n",
      "Transform train X-(60000, 64) to 26-PCA Time: 0.012821 sec\n",
      "Transform test X-(60000, 64) to 26-PCA Time: 0.002087 sec\n"
     ]
    }
   ],
   "source": [
    "pca_dims, pca_train_features, pca_test_features, pca_fit_time, pca_train_features_extract_time, pca_test_features_extract_time = \\\n",
    "    pca_run(train_data_X = train_features,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0080 sec\n",
      "Train Accuracy : 94.62 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 95.35 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(pca_train_features, \n",
    "                train_targets, \n",
    "                pca_test_features, \n",
    "                test_targets,\n",
    "                config = config,\n",
    "                metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 43 PCA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: PCA\n",
      "│    └────Component size: 43\n",
      "│    └────Feature Reduction Ratio: 5.5%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['PCA', 43]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 43\n",
      "\n",
      "Calculating SVD Matrix Time on Train Data-(60000, 64) : 0.156589 sec\n",
      "Transform train X-(60000, 64) to 43-PCA Time: 0.012767 sec\n",
      "Transform test X-(60000, 64) to 43-PCA Time: 0.002439 sec\n"
     ]
    }
   ],
   "source": [
    "pca_dims, pca_train_features, pca_test_features, pca_fit_time, pca_train_features_extract_time, pca_test_features_extract_time = \\\n",
    "    pca_run(train_data_X = train_features,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0090 sec\n",
      "Train Accuracy : 94.62 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0020 sec\n",
      "Test Accuracy : 95.36 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(pca_train_features, \n",
    "                train_targets, \n",
    "                pca_test_features, \n",
    "                test_targets,\n",
    "                config = config,\n",
    "                metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN feature + LDA + NCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_run(train_data_X,\n",
    "            train_data_y,\n",
    "            test_data_X,\n",
    "            config=None):\n",
    "\n",
    "    assert config is not None\n",
    "#     config.info()\n",
    "\n",
    "    n_components = config.reduction_method[1]\n",
    "\n",
    "    lda_dims = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "    print(f\"The number of components : {n_components}\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    lda_dims.fit(train_data_X, train_data_y)\n",
    "    lda_fit_time = time.perf_counter () - start_time\n",
    "    print()\n",
    "    print(f\"Calculating LDA Matrix Time on Train Data-{train_data_X.shape} : {lda_fit_time:4f} sec\")\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    train_features = lda_dims.transform(train_data_X)\n",
    "    train_features_extract_time = time.perf_counter () - start_time\n",
    "    print(f\"Transform train X-{train_data_X.shape} to {n_components}-LDA Time: {train_features_extract_time:4f} sec\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_features = lda_dims.transform(test_data_X)\n",
    "    test_features_extract_time = time.perf_counter () - start_time\n",
    "    print(f\"Transform test X-{train_data_X.shape} to {n_components}-LDA Time: {test_features_extract_time:4f} sec\")\n",
    "\n",
    "    return lda_dims, train_features, test_features, lda_fit_time, train_features_extract_time, test_features_extract_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 2 LDA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: LDA\n",
      "│    └────Component size: 2\n",
      "│    └────Feature Reduction Ratio: 0.3%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['LDA', 2]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 2\n",
      "\n",
      "Calculating LDA Matrix Time on Train Data-(60000, 64) : 0.266032 sec\n",
      "Transform train X-(60000, 64) to 2-LDA Time: 0.012951 sec\n",
      "Transform test X-(60000, 64) to 2-LDA Time: 0.003570 sec\n"
     ]
    }
   ],
   "source": [
    "lda_dims, lda_train_features, lda_test_features, lda_fit_time, lda_train_features_extract_time, lda_test_features_extract_time = \\\n",
    "    lda_run(train_data_X = train_features,\n",
    "            train_data_y = train_targets,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0070 sec\n",
      "Train Accuracy : 80.80 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 81.45 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(lda_train_features, \n",
    "                train_targets, \n",
    "                lda_test_features, \n",
    "                test_targets,\n",
    "                config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 3 LDA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: LDA\n",
      "│    └────Component size: 3\n",
      "│    └────Feature Reduction Ratio: 0.4%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['LDA', 3]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 3\n",
      "\n",
      "Calculating LDA Matrix Time on Train Data-(60000, 64) : 0.269271 sec\n",
      "Transform train X-(60000, 64) to 3-LDA Time: 0.012475 sec\n",
      "Transform test X-(60000, 64) to 3-LDA Time: 0.003209 sec\n"
     ]
    }
   ],
   "source": [
    "lda_dims, lda_train_features, lda_test_features, lda_fit_time, lda_train_features_extract_time, lda_test_features_extract_time = \\\n",
    "    lda_run(train_data_X = train_features,\n",
    "            train_data_y = train_targets,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0080 sec\n",
      "Train Accuracy : 89.33 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 89.49 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(lda_train_features, \n",
    "                train_targets, \n",
    "                lda_test_features, \n",
    "                test_targets,\n",
    "                config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 5 LDA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: LDA\n",
      "│    └────Component size: 5\n",
      "│    └────Feature Reduction Ratio: 0.6%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['LDA', 5]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 5\n",
      "\n",
      "Calculating LDA Matrix Time on Train Data-(60000, 64) : 0.264045 sec\n",
      "Transform train X-(60000, 64) to 5-LDA Time: 0.018373 sec\n",
      "Transform test X-(60000, 64) to 5-LDA Time: 0.002920 sec\n"
     ]
    }
   ],
   "source": [
    "lda_dims, lda_train_features, lda_test_features, lda_fit_time, lda_train_features_extract_time, lda_test_features_extract_time = \\\n",
    "    lda_run(train_data_X = train_features,\n",
    "            train_data_y = train_targets,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0070 sec\n",
      "Train Accuracy : 95.62 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 95.59 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(lda_train_features, \n",
    "                train_targets, \n",
    "                lda_test_features, \n",
    "                test_targets,\n",
    "                config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN features - 9 LDA - NCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ── PC\n",
      "│\n",
      "├──Dataset\n",
      "│    └────MNIST\n",
      "│    └────Train size 80%\n",
      "│    └────Feature size: 784\n",
      "│\n",
      "├──Method\n",
      "│    └────NCM\n",
      "│    └────Euclidean\n",
      "│\n",
      "├──Dimension reduction\n",
      "│    └────Method: LDA\n",
      "│    └────Component size: 9\n",
      "│    └────Feature Reduction Ratio: 1.0999999999999999%\n",
      "│\n",
      "└──Iteration\n",
      "    └────10\n"
     ]
    }
   ],
   "source": [
    "config.reduction_method = ['LDA', 9]\n",
    "config.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of components : 9\n",
      "\n",
      "Calculating LDA Matrix Time on Train Data-(60000, 64) : 0.267432 sec\n",
      "Transform train X-(60000, 64) to 9-LDA Time: 0.011592 sec\n",
      "Transform test X-(60000, 64) to 9-LDA Time: 0.002387 sec\n"
     ]
    }
   ],
   "source": [
    "lda_dims, lda_train_features, lda_test_features, lda_fit_time, lda_train_features_extract_time, lda_test_features_extract_time = \\\n",
    "    lda_run(train_data_X = train_features,\n",
    "            train_data_y = train_targets,\n",
    "            test_data_X = test_features,\n",
    "            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fit\n",
      "\n",
      "Train Data Feature Extract Time : 4.2202 sec\n",
      "Train Data Fitting NCM Time : 0.0080 sec\n",
      "Train Accuracy : 98.28 %\n",
      "\n",
      "Finished Predicting\n",
      "\n",
      "Test Data Feature Extract Time : 1.3960 sec\n",
      "Test Data Prediction Time : 0.0010 sec\n",
      "Test Accuracy : 98.24 %\n"
     ]
    }
   ],
   "source": [
    "acc, test_acc, feature_extract_time, test_feature_extract_time = \\\n",
    "    cnn_ncm_run(lda_train_features, \n",
    "                train_targets, \n",
    "                lda_test_features, \n",
    "                test_targets,\n",
    "                config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96617a4ccdf4ff8af9cb4fdc6a0e520bbd799675b4cf9420cbe414d64a946884"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
